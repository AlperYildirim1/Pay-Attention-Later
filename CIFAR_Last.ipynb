{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNAo8vHhtYtH3HE1BxKNH9w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Language-as-Waves/blob/main/CIFAR_Last.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION (UNCHANGED)\n",
        "# ==========================================\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20 # Reduced for ablation speed (Increase to 50 for full paper results)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SLM_BITS = 4\n",
        "\n",
        "# Learning Rates\n",
        "LR_OPTICAL = 0.005\n",
        "LR_DIGITAL = 0.001\n",
        "\n",
        "print(f\"‚öôÔ∏è DEVICE: {DEVICE}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PREPARATION (UNCHANGED)\n",
        "# ==========================================\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# ==========================================\n",
        "# 3. CORE LOGIC (EXACT COPY OF YOUR CODE)\n",
        "# ==========================================\n",
        "def quantize_slm(x, bits):\n",
        "    levels = 2**bits - 1\n",
        "    return torch.round(x * levels) / levels\n",
        "\n",
        "class StableOpticalLayer(nn.Module):\n",
        "    def __init__(self, num_filters):\n",
        "        super().__init__()\n",
        "        # Initialization: Balanced magnitude, small random phase\n",
        "        self.slm_mag = nn.Parameter(torch.ones(num_filters, 32, 32) * 0.8)\n",
        "        self.slm_phase = nn.Parameter(torch.randn(num_filters, 32, 32) * 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.view(B * C, 1, H, W)\n",
        "\n",
        "        # Physical Constraints\n",
        "        mag = torch.sigmoid(self.slm_mag)\n",
        "        mag = quantize_slm(mag, SLM_BITS)\n",
        "        phase = self.slm_phase\n",
        "\n",
        "        # Fourier Optics (Stable Autograd Version)\n",
        "        x_fft = torch.fft.fftshift(torch.fft.fft2(x))\n",
        "        h = mag * torch.exp(1j * phase)\n",
        "        obj_fft = x_fft * h\n",
        "        out_spatial = torch.fft.ifft2(torch.fft.ifftshift(obj_fft))\n",
        "\n",
        "        # Intensity Detection\n",
        "        out_intensity = out_spatial.abs()\n",
        "\n",
        "        # Hardware Noise Simulation (Training Only)\n",
        "        if self.training:\n",
        "            out_intensity = out_intensity * (1.0 + torch.randn_like(out_intensity) * 0.01)\n",
        "\n",
        "        # Stable AGC (Automatic Gain Control)\n",
        "        max_val = torch.amax(out_intensity, dim=(2,3), keepdim=True) + 1e-6\n",
        "        out_normalized = out_intensity / max_val\n",
        "\n",
        "        return out_normalized\n",
        "\n",
        "class HybridNetFinal(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.optics = StableOpticalLayer(num_filters=64)\n",
        "\n",
        "        self.pool_dim = 8\n",
        "        self.flat_dim = 64 * 3 * self.pool_dim**2\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.flat_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        opt_out = self.optics(x)\n",
        "        pooled = F.adaptive_max_pool2d(opt_out, (self.pool_dim, self.pool_dim))\n",
        "        flat = pooled.view(B, -1)\n",
        "        return self.classifier(flat)\n",
        "\n",
        "# ==========================================\n",
        "# 4. ABLATION ENGINE\n",
        "# ==========================================\n",
        "def run_training_session(mode_name, freeze_optics):\n",
        "    \"\"\"\n",
        "    Runs a training session.\n",
        "    freeze_optics (bool): If True, optics layer is NOT optimized (random projection).\n",
        "    \"\"\"\n",
        "    print(f\"\\nüöÄ STARTING EXPERIMENT: {mode_name}\")\n",
        "    print(f\"   Optics Frozen: {freeze_optics}\")\n",
        "\n",
        "    model = HybridNetFinal().to(DEVICE)\n",
        "\n",
        "    # --- CRITICAL: OPTIMIZER CONFIGURATION ---\n",
        "    if freeze_optics:\n",
        "        # OPTION B: FROZEN OPTICS (Baseline)\n",
        "        # We only pass classifier parameters. Optics remains random.\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': model.classifier.parameters(), 'lr': LR_DIGITAL}\n",
        "        ], weight_decay=1e-3)\n",
        "    else:\n",
        "        # OPTION A: LEARNED OPTICS (Proposed Method)\n",
        "        # We optimize both optics (slowly) and classifier.\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': model.optics.parameters(), 'lr': LR_OPTICAL},\n",
        "            {'params': model.classifier.parameters(), 'lr': LR_DIGITAL}\n",
        "        ], weight_decay=1e-3)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    acc_history = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Testing\n",
        "        model.eval()\n",
        "        test_correct = 0\n",
        "        total_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(DEVICE), target.to(DEVICE)\n",
        "                output = model(data)\n",
        "                pred = output.argmax(1)\n",
        "                test_correct += pred.eq(target).sum().item()\n",
        "                total_samples += target.size(0)\n",
        "\n",
        "        acc = 100. * test_correct / total_samples\n",
        "        acc_history.append(acc)\n",
        "        print(f\"   Epoch {epoch+1}/{EPOCHS} | Test Acc: {acc:.2f}%\")\n",
        "\n",
        "    return acc_history\n",
        "\n",
        "# ==========================================\n",
        "# 5. EXECUTION & PLOTTING\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Experiment 1: Full Model (Optics Learning)\n",
        "    acc_learned = run_training_session(\"PROPOSED (Learned Optics)\", freeze_optics=False)\n",
        "\n",
        "    # Experiment 2: Baseline (Optics Frozen / Random)\n",
        "    acc_frozen = run_training_session(\"BASELINE (Frozen Optics)\", freeze_optics=True)\n",
        "\n",
        "    # --- PLOTTING FOR PAPER ---\n",
        "    plt.figure(figsize=(10, 6), dpi=120)\n",
        "    epochs_x = range(1, EPOCHS + 1)\n",
        "\n",
        "    plt.plot(epochs_x, acc_learned, 'o-', label='Learned Optics (Proposed)', color='blue', linewidth=2)\n",
        "    plt.plot(epochs_x, acc_frozen, 'x--', label='Frozen Optics (Random Baseline)', color='gray', linewidth=2)\n",
        "\n",
        "    plt.xlabel('Epochs', fontsize=12)\n",
        "    plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    plt.title('Ablation Study: Contribution of Optical Learning', fontsize=14)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Calculate Delta\n",
        "    final_delta = acc_learned[-1] - acc_frozen[-1]\n",
        "    print(f\"\\nüìä FINAL RESULTS:\")\n",
        "    print(f\"   Learned Accuracy: {acc_learned[-1]:.2f}%\")\n",
        "    print(f\"   Frozen Accuracy:  {acc_frozen[-1]:.2f}%\")\n",
        "    print(f\"   Optical Gain (Delta): +{final_delta:.2f}%\")\n",
        "\n",
        "    if final_delta > 3.0:\n",
        "        print(\"‚úÖ CONCLUSION: The optical layer provides significant learnable features.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è CONCLUSION: The digital head dominates performance (Consider reducing classifier size).\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "tL0eMdwLr7kx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}