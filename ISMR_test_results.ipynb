{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "mount_file_id": "1ENCNDrQI7GAc2sLc7aM4ooSVxybsUn5f",
      "authorship_tag": "ABX9TyNqsbviavyD2/BZmVZMlHjs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/ISMR_test_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchmetrics sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVLoQ6awjxot",
        "outputId": "e4757175-a43d-4a22-fd9f-b3537e006879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYisVeLAjZhI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "from torchmetrics.text import BLEUScore\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 1. GLOBAL CONFIGURATIONS ---\n",
        "# ==============================================================================\n",
        "print(\"--- Loading Global Configurations ---\")\n",
        "DRIVE_BASE_PATH = \"/content/drive/MyDrive\"\n",
        "ORIGINAL_BUCKETED_REPO_ID = \"Yujivus/multi30k-de-en-bucketed-w4\"\n",
        "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-de-en\" # Tokenizer source\n",
        "\n",
        "# --- Model Hyperparameters (Must match training) ---\n",
        "MAX_LENGTH = 128\n",
        "D_MODEL = 512\n",
        "NUM_HEADS = 8\n",
        "D_FF = 2048\n",
        "DROPOUT = 0.1\n",
        "EVAL_BATCH_SIZE = 64\n",
        "SEED_FOR_DATALOADER = 115 # Just needs to be consistent\n",
        "\n",
        "# --- Device Setup ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 2. DATA LOADER SETUP ---\n",
        "# ==============================================================================\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    \"\"\"Sets seed for DataLoader workers for reproducibility.\"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "\n",
        "print(\"--- Initializing Tokenizer and DataLoaders ---\")\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
        "\n",
        "# Load Original Datasets (for 'test' split)\n",
        "try:\n",
        "    original_datasets = load_dataset(ORIGINAL_BUCKETED_REPO_ID)\n",
        "except Exception as e:\n",
        "    print(f\"CRITICAL: Could not load dataset. Check HF connection or path: {e}\")\n",
        "    # This is a fatal error, but we'll let it crash later if 'setup_test_dataloader' is called\n",
        "\n",
        "# Define the standard collator\n",
        "standard_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
        "\n",
        "# Define a consistent generator for the DataLoader\n",
        "g = torch.Generator()\n",
        "g.manual_seed(SEED_FOR_DATALOADER)\n",
        "\n",
        "def setup_test_dataloader():\n",
        "    \"\"\"Sets up the DataLoader for the test set.\"\"\"\n",
        "    print(f\"Loading 'test' split from: {ORIGINAL_BUCKETED_REPO_ID}\")\n",
        "    try:\n",
        "        test_dataset = original_datasets[\"test\"]\n",
        "        test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=EVAL_BATCH_SIZE,\n",
        "            collate_fn=standard_collator,\n",
        "            num_workers=0,\n",
        "            pin_memory=True,\n",
        "            worker_init_fn=seed_worker,\n",
        "            generator=g,\n",
        "        )\n",
        "        print(\"Test dataloader created successfully.\")\n",
        "        return test_dataloader\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR: Failed to create test dataloader. {e}\")\n",
        "        raise\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 3. MODEL ARCHITECTURE (Must match training) ---\n",
        "# ==============================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Injects positional information into the input embeddings.\"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(1, max_len, d_model)\n",
        "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
        "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"A standard two-layer feed-forward network with a ReLU activation.\"\"\"\n",
        "    def __init__(self, d_model: int, dff: int, dropout_rate: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dff, d_model),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.ffn(x)\n",
        "\n",
        "class StandardTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers, num_heads, d_model, dff, vocab_size, max_length, dropout):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_length)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model, num_heads, dff, dropout, batch_first=True, norm_first=True # <-- THE FIX\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model, num_heads, dff, dropout, batch_first=True, norm_first=True # <-- THE FIX\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "        self.final_linear.weight = self.embedding.weight\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask, memory_key_padding_mask, tgt_mask):\n",
        "\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        src_emb_pos = self.dropout(self.pos_encoder(src_emb))\n",
        "        tgt_emb_pos = self.dropout(self.pos_encoder(tgt_emb))\n",
        "\n",
        "        memory = self.encoder(src_emb_pos, src_key_padding_mask=src_padding_mask)\n",
        "        decoder_output = self.decoder(\n",
        "            tgt=tgt_emb_pos, memory=memory, tgt_mask=tgt_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask, memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "        return self.final_linear(decoder_output)\n",
        "\n",
        "\n",
        "    def create_masks(self, src, tgt):\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "        tgt_padding_mask = (tgt == tokenizer.pad_token_id)\n",
        "        # Creates a square causal mask for the decoder. This prevents any token from attending to future tokens. With this way model can not cheat.\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n",
        "            sz=tgt.size(1),\n",
        "            device=src.device,\n",
        "            dtype=torch.bool\n",
        "        )\n",
        "        return src_padding_mask, tgt_padding_mask, src_padding_mask, tgt_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src: torch.Tensor, max_length: int, num_beams: int = 5) -> torch.Tensor:\n",
        "        self.eval()\n",
        "        src_padding_mask = (src == tokenizer.pad_token_id)\n",
        "\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src_emb_pos = self.pos_encoder(src_emb)\n",
        "        memory = self.encoder(self.dropout(src_emb_pos), src_key_padding_mask=src_padding_mask)\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        memory = memory.repeat_interleave(num_beams, dim=0)\n",
        "        memory_key_padding_mask = src_padding_mask.repeat_interleave(num_beams, dim=0)\n",
        "\n",
        "        initial_token = tokenizer.pad_token_id\n",
        "        beams = torch.full((batch_size * num_beams, 1), initial_token, dtype=torch.long, device=src.device)\n",
        "\n",
        "        beam_scores = torch.zeros(batch_size * num_beams, device=src.device)\n",
        "        finished_beams = torch.zeros(batch_size * num_beams, dtype=torch.bool, device=src.device)\n",
        "        for _ in range(max_length - 1):\n",
        "            if finished_beams.all(): break\n",
        "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(beams.size(1)).to(src.device)\n",
        "            tgt_emb = self.embedding(beams) * math.sqrt(self.d_model) # FIX HERE TOO\n",
        "            tgt_emb_pos = self.pos_encoder(tgt_emb)\n",
        "            decoder_output = self.decoder(tgt=self.dropout(tgt_emb_pos), memory=memory, tgt_mask=tgt_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
        "            logits = self.final_linear(decoder_output[:, -1, :])\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            log_probs[:, tokenizer.pad_token_id] = -torch.inf\n",
        "            if finished_beams.any(): log_probs[finished_beams, tokenizer.eos_token_id] = 0\n",
        "            total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "            if _ == 0:\n",
        "                total_scores = total_scores.view(batch_size, num_beams, -1)\n",
        "                total_scores[:, 1:, :] = -torch.inf # Sadece ilk beam'in başlamasına izin ver\n",
        "                total_scores = total_scores.view(batch_size * num_beams, -1)\n",
        "            else:\n",
        "                total_scores = beam_scores.unsqueeze(1) + log_probs\n",
        "            total_scores = total_scores.view(batch_size, -1)\n",
        "            top_scores, top_indices = torch.topk(total_scores, k=num_beams, dim=1)\n",
        "            beam_indices = top_indices // log_probs.shape[-1]; token_indices = top_indices % log_probs.shape[-1]\n",
        "            batch_indices = torch.arange(batch_size, device=src.device).unsqueeze(1)\n",
        "            effective_indices = (batch_indices * num_beams + beam_indices).view(-1)\n",
        "            beams = beams[effective_indices]\n",
        "            beams = torch.cat([beams, token_indices.view(-1, 1)], dim=1)\n",
        "            beam_scores = top_scores.view(-1)\n",
        "            finished_beams = finished_beams | (beams[:, -1] == tokenizer.eos_token_id)\n",
        "        final_beams = beams.view(batch_size, num_beams, -1)\n",
        "        final_scores = beam_scores.view(batch_size, num_beams)\n",
        "        normalized_scores = final_scores / (final_beams != tokenizer.pad_token_id).sum(-1).float().clamp(min=1)\n",
        "        best_beams = final_beams[torch.arange(batch_size), normalized_scores.argmax(1), :]\n",
        "        self.train()\n",
        "        return best_beams\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 4. EVALUATION FUNCTION (Must match training) ---\n",
        "# ==============================================================================\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"Evaluates the model using beam search decoding.\"\"\"\n",
        "    bleu_metric = BLEUScore()\n",
        "    model.eval() # Ensure model is in eval mode\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'] # Keep labels on CPU for sacrebleu\n",
        "\n",
        "        generated_ids = model.generate(input_ids, max_length=MAX_LENGTH, num_beams=5)\n",
        "\n",
        "        # Decode predictions and references\n",
        "        pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        # Replace -100 in labels (which are on CPU) before decoding\n",
        "        labels[labels == -100] = tokenizer.pad_token_id\n",
        "        ref_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        # sacrebleu expects references to be in a list of lists\n",
        "        bleu_metric.update(pred_texts, [[ref] for ref in ref_texts])\n",
        "\n",
        "    return bleu_metric.compute().item() * 100 # Return BLEU score as 0-100\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 5. EXPERIMENT DEFINITIONS ---\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Model Architecture Configs ---\n",
        "STD_TRANSFORMER_CONFIG = {\n",
        "    \"num_encoder_layers\": 20,\n",
        "    \"num_decoder_layers\": 20,\n",
        "    \"num_heads\": NUM_HEADS,\n",
        "    \"d_model\": D_MODEL,\n",
        "    \"dff\": D_FF,\n",
        "    \"vocab_size\": VOCAB_SIZE,\n",
        "    \"max_length\": MAX_LENGTH,\n",
        "    \"dropout\": DROPOUT\n",
        "}\n",
        "\n",
        "WIDE_TRANSFORMER_CONFIG = {\n",
        "    \"num_encoder_layers\": 1,\n",
        "    \"num_decoder_layers\": 1,\n",
        "    \"num_heads\": NUM_HEADS,\n",
        "    \"d_model\": D_MODEL,\n",
        "    \"dff\": D_FF,\n",
        "    \"vocab_size\": VOCAB_SIZE,\n",
        "    \"max_length\": MAX_LENGTH,\n",
        "    \"dropout\": DROPOUT\n",
        "}\n",
        "\n",
        "# --- Programmatically generate all experiment paths ---\n",
        "SEEDS = [115, 116, 117, 118]\n",
        "experiments = {}\n",
        "\n",
        "for seed in SEEDS:\n",
        "    # --- 20-Layer ---\n",
        "    experiments[f\"20-Layer (Seed {seed}) - Baseline\"] = {\n",
        "        \"path\": f\"iterative/iterative-30k-dataset-seed-{seed}-20-layered-transformer-1e-4/iter_1\",\n",
        "        \"config\": STD_TRANSFORMER_CONFIG,\n",
        "        \"group\": \"20-Layer Baseline\"\n",
        "    }\n",
        "    experiments[f\"20-Layer (Seed {seed}) - ISMR\"] = {\n",
        "        \"path\": f\"iterative/iterative-30k-dataset-seed-{seed}-20-layered-transformer-1e-4/iter_2\",\n",
        "        \"config\": STD_TRANSFORMER_CONFIG,\n",
        "        \"group\": \"20-Layer ISMR\"\n",
        "    }\n",
        "    experiments[f\"20-Layer (Seed {seed}) - Ablation\"] = {\n",
        "        \"path\": f\"iterative/iterative-30k-dataset-seed-{seed}-SHUFFLED-ABLATION-1e-4/iter_1_shuffled\",\n",
        "        \"config\": STD_TRANSFORMER_CONFIG,\n",
        "        \"group\": \"20-Layer Ablation\"\n",
        "    }\n",
        "    # --- 1-Layer ---\n",
        "    experiments[f\"1-Layer (Seed {seed}) - Baseline\"] = {\n",
        "        \"path\": f\"iterative/iterative-30k-dataset-seed-{seed}-1-layered-transformer-1e-4/iter_1\",\n",
        "        \"config\": WIDE_TRANSFORMER_CONFIG,\n",
        "        \"group\": \"1-Layer Baseline\"\n",
        "    }\n",
        "    experiments[f\"1-Layer (Seed {seed}) - ISMR\"] = {\n",
        "        \"path\": f\"iterative/iterative-30k-dataset-seed-{seed}-1-layered-transformer-1e-4/iter_2\",\n",
        "        \"config\": WIDE_TRANSFORMER_CONFIG,\n",
        "        \"group\": \"1-Layer ISMR\"\n",
        "    }\n",
        "\n",
        "print(f\"--- Defined {len(experiments)} total experiments to evaluate ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 6. MAIN EVALUATION LOOP ---\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n--- Starting Final Evaluation on Test Set ---\")\n",
        "\n",
        "# --- Setup Test Dataloader (once) ---\n",
        "test_dataloader = setup_test_dataloader()\n",
        "results_list = []\n",
        "\n",
        "for name, details in experiments.items():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"--- Evaluating Experiment: {name} ---\")\n",
        "    print(f\"    Group: {details['group']}\")\n",
        "\n",
        "    # --- 1. Define Path to Best Model ---\n",
        "    iter_folder = details[\"path\"] # Get the base path from the dictionary\n",
        "\n",
        "    # *** NEW FALLBACK LOGIC (from plot_averages.py) ***\n",
        "    # Check if this is an ablation run and if the 'iter_1_shuffled' path exists\n",
        "    if details['group'] == \"20-Layer Ablation\":\n",
        "        base_run_path = details[\"path\"].rsplit('/', 1)[0] # Get the parent folder\n",
        "        shuffled_path = os.path.join(DRIVE_BASE_PATH, base_run_path, \"iter_1_shuffled\")\n",
        "        if not os.path.exists(shuffled_path):\n",
        "            print(f\"    [INFO] 'iter_1_shuffled' not found. Trying 'iter_1' instead.\")\n",
        "            # Re-point iter_folder to the 'iter_1' version\n",
        "            iter_folder = os.path.join(base_run_path, \"iter_1\")\n",
        "    # *** END OF NEW LOGIC ***\n",
        "\n",
        "    model_relative_path = os.path.join(iter_folder, \"models\", \"best.pt\")\n",
        "    final_best_model_path = os.path.join(DRIVE_BASE_PATH, model_relative_path)\n",
        "    print(f\"    Loading model from: {final_best_model_path}\")\n",
        "\n",
        "    if not os.path.exists(final_best_model_path):\n",
        "        print(f\"!!! WARNING: Model file not found. Skipping this experiment.\")\n",
        "        results_list.append({\n",
        "            \"Experiment\": name,\n",
        "            \"Group\": details['group'],\n",
        "            \"Test BLEU\": np.nan,\n",
        "            \"Error\": \"Model file not found\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # --- 2. Instantiate Model ---\n",
        "    print(\"    Instantiating model...\")\n",
        "    try:\n",
        "        model_config = details[\"config\"]\n",
        "        final_model = StandardTransformer(**model_config)\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR: Failed to instantiate model. {e}\")\n",
        "        results_list.append({\n",
        "            \"Experiment\": name,\n",
        "            \"Group\": details['group'],\n",
        "            \"Test BLEU\": np.nan,\n",
        "            \"Error\": \"Model instantiation error\"\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # --- 3. Load Weights ---\n",
        "    try:\n",
        "        final_model.load_state_dict(torch.load(final_best_model_path, map_location=device))\n",
        "        print(\"    Successfully loaded model weights.\")\n",
        "        final_model.to(device)\n",
        "        final_model.eval()\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR: Failed to load model weights. {e}\")\n",
        "        results_list.append({\n",
        "            \"Experiment\": name,\n",
        "            \"Group\": details['group'],\n",
        "            \"Test BLEU\": np.nan,\n",
        "            \"Error\": \"Weight loading error\"\n",
        "        })\n",
        "        del final_model\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # --- 4. Run Evaluation ---\n",
        "    print(\"    Running evaluation on the test set...\")\n",
        "    try:\n",
        "        test_bleu_score = evaluate(final_model, test_dataloader, device)\n",
        "        print(f\"    --> Test Set BLEU Score: {test_bleu_score:.4f}\")\n",
        "        results_list.append({\n",
        "            \"Experiment\": name,\n",
        "            \"Group\": details['group'],\n",
        "            \"Test BLEU\": test_bleu_score,\n",
        "            \"Error\": None\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR: Evaluation failed. {e}\")\n",
        "        results_list.append({\n",
        "            \"Experiment\": name,\n",
        "            \"Group\": details['group'],\n",
        "            \"Test BLEU\": np.nan,\n",
        "            \"Error\": f\"Evaluation runtime error: {e}\"\n",
        "        })\n",
        "\n",
        "    # --- 5. Clean up ---\n",
        "    del final_model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# ==============================================================================\n",
        "# --- 7. FINAL REPORT ---\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"--- FINAL TEST SET RESULTS (ALL RUNS) ---\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "df_results = pd.DataFrame(results_list)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(df_results.to_string(index=False, columns=[\"Group\", \"Experiment\", \"Test BLEU\", \"Error\"]))\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*70)\n",
        "print(\"--- FINAL AVERAGED RESULTS SUMMARY ---\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Define a specific order for the final table\n",
        "group_order = [\n",
        "    \"20-Layer Baseline\",\n",
        "    \"20-Layer ISMR\",\n",
        "    \"20-Layer Ablation\",\n",
        "    \"1-Layer Baseline\",\n",
        "    \"1-Layer ISMR\"\n",
        "]\n",
        "\n",
        "# Calculate statistics\n",
        "avg_results = df_results.groupby('Group')['Test BLEU'].agg(['mean', 'std', 'count']).reindex(group_order)\n",
        "print(avg_results.to_string(float_format=\"%.4f\"))\n",
        "\n",
        "print(\"\\n\\nEvaluation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "yTS7BLRrowdk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}