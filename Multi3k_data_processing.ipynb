{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPASQEJUk38+GygHoLIHLbU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlperYildirim1/Pay-Attention-Later/blob/main/Multi3k_data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8aIUNKBEVBe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import login\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# --- 2. CONFIGURATION (Edit this section only) ---\n",
        "\n",
        "HF_USERNAME = \"Your Username\"\n",
        "\n",
        "# --- General Settings ---\n",
        "# This now points to YOUR STABLE copy of the dataset.\n",
        "BASE_DATASET_ID = f\"Yujivus/multi30k-de-en-3k-subset\"\n",
        "LANGUAGE_PAIR = \"de-en\"\n",
        "MODEL_CHECKPOINT = \"Helsinki-NLP/opus-mt-de-en\"\n",
        "REPRODUCIBILITY_SEED = 42\n",
        "\n",
        "# --- Processing Settings ---\n",
        "MAX_LENGTH = 128\n",
        "BUCKET_WIDTH = 4\n",
        "TOKEN_LIMIT_PER_BATCH = 500\n",
        "\n",
        "# --- Automatic Repository Naming (Do not touch) ---\n",
        "BASE_NAME = \"multi3k-de-en\"\n",
        "BUCKETED_REPO_NAME = f\"{BASE_NAME}-bucketed-w{BUCKET_WIDTH}\"\n",
        "PREBATCHED_REPO_NAME = f\"{BASE_NAME}-prebatched-w{BUCKET_WIDTH}\"\n",
        "\n",
        "print(\"--- Configuration ---\")\n",
        "print(f\"Target Dataset: {BASE_DATASET_ID}\")\n",
        "print(f\"Bucketed Repo to be created: {HF_USERNAME}/{BUCKETED_REPO_NAME}\")\n",
        "print(f\"Pre-batched Repo to be created: {HF_USERNAME}/{PREBATCHED_REPO_NAME}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "login(\"...\")\n",
        "\n",
        "print(\"\\n--- Step 1: Loading Raw Dataset ---\")\n",
        "raw_dataset = load_dataset(BASE_DATASET_ID)\n",
        "print(f\" Raw dataset loaded successfully: {raw_dataset}\")\n",
        "\n",
        "# --- 4. TOKENIZE, BUCKET, AND SORT ---\n",
        "\n",
        "print(\"\\n--- Step 2: Tokenizing, Bucketing, and Sorting ---\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def process_function(examples):\n",
        "    # This function is adapted for the flat column names (\"de\", \"en\") of Multi30k.\n",
        "    inputs = examples[\"de\"]\n",
        "    targets = examples[\"en\"]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_LENGTH, truncation=True, padding=False)\n",
        "    labels = tokenizer(text_target=targets, max_length=MAX_LENGTH, truncation=True, padding=False)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    model_inputs[\"input_length\"] = [len(x) for x in model_inputs[\"input_ids\"]]\n",
        "    model_inputs[\"labels_length\"] = [len(x) for x in model_inputs[\"labels\"]]\n",
        "\n",
        "    # Add bucket_id simultaneously\n",
        "    costs = [max(i_len, l_len) for i_len, l_len in zip(model_inputs[\"input_length\"], model_inputs[\"labels_length\"])]\n",
        "    model_inputs[\"bucket_id\"] = [cost // BUCKET_WIDTH for cost in costs]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "processed_dataset = raw_dataset.map(\n",
        "    process_function,\n",
        "    batched=True,\n",
        "    num_proc=os.cpu_count() or 2,\n",
        "    remove_columns=raw_dataset[\"train\"].column_names,\n",
        "    desc=\"Tokenize & Bucket\"\n",
        ")\n",
        "\n",
        "print(\"Sorting...\")\n",
        "# We only need to sort the train split.\n",
        "processed_dataset['train'] = processed_dataset['train'].sort(\"bucket_id\")\n",
        "\n",
        "print(f\" Processing complete. Uploading dataset to '{HF_USERNAME}/{BUCKETED_REPO_NAME}'...\")\n",
        "processed_dataset.push_to_hub(f\"{HF_USERNAME}/{BUCKETED_REPO_NAME}\", private=False)\n",
        "print(\" Bucketed dataset uploaded successfully.\")\n",
        "\n",
        "# --- 5. PRE-BATCHING ---\n",
        "\n",
        "print(f\"\\n--- Step 3: Pre-batching and creating '{HF_USERNAME}/{PREBATCHED_REPO_NAME}' ---\")\n",
        "train_split_for_batching = processed_dataset['train']\n",
        "\n",
        "bucket_ids = np.array(train_split_for_batching['bucket_id'], dtype=np.int32)\n",
        "lengths = np.maximum(\n",
        "    np.array(train_split_for_batching['input_length'], dtype=np.int32),\n",
        "    np.array(train_split_for_batching['labels_length'], dtype=np.int32)\n",
        ")\n",
        "\n",
        "boundaries = np.where(np.diff(bucket_ids) != 0)[0] + 1\n",
        "bucket_indices_list = np.split(np.arange(len(bucket_ids)), boundaries)\n",
        "\n",
        "all_batches = []\n",
        "for bucket_indices in tqdm(bucket_indices_list, desc=\"Creating batches\"):\n",
        "    if len(bucket_indices) == 0: continue\n",
        "    max_len_in_bucket = np.max(lengths[bucket_indices])\n",
        "    samples_per_batch = max(1, TOKEN_LIMIT_PER_BATCH // (max_len_in_bucket + 1))\n",
        "    for j in range(0, len(bucket_indices), int(samples_per_batch)):\n",
        "        batch = bucket_indices[j : j + samples_per_batch].tolist()\n",
        "        if batch: all_batches.append(batch)\n",
        "\n",
        "prebatched_train_split = Dataset.from_dict({\"batch_indices\": all_batches})\n",
        "\n",
        "print(f\" Pre-batching complete. Uploading dataset to '{HF_USERNAME}/{PREBATCHED_REPO_NAME}'...\")\n",
        "prebatched_train_split.push_to_hub(f\"{HF_USERNAME}/{PREBATCHED_REPO_NAME}\", split=\"train\", private=False)\n",
        "print(\" Pre-batched (train split) dataset uploaded successfully.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# --- FINAL RESULT ---\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ALL STEPS COMPLETED SUCCESSFULLY! \")\n",
        "print(\"\\nRepository IDs to use in your training script:\")\n",
        "print(f'PREBATCHED_REPO_ID = \"{HF_USERNAME}/{PREBATCHED_REPO_NAME}\"')\n",
        "print(f'ORIGINAL_BUCKETED_REPO_ID = \"{HF_USERNAME}/{BUCKETED_REPO_NAME}\"')\n",
        "print(\"=\"*60)"
      ]
    }
  ]
}